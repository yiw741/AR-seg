# Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos

# 高效语义分割通过改变压缩视频的分辨率

为了实现计算高效的`VSS`（视频语义分割），提出了**紧凑型网络架构（减少模型参数）**或**自适应网络策略（动态调整计算过程如结构，层数）**等。然而这些工作都没有考虑到影响计算成本的一个关键因素：**输入分辨率**。（通过下采样进行减少参数动态，动态改变结构，使得模型参数少的同时也有较多信息）



在高帧率视频中，**输入分辨率**指的是视频源的**像素尺寸**。高帧率视频通常指的是如24帧/秒或30帧/秒

- **1080p（1920×1080）**：常用于60帧/秒的高帧率视频。
- **4K（3840×2160）**：可以支持60帧/秒或更高的帧率，适合高质量视频制作。
- **8K（7680×4320）**：在一些专业应用中，可能会使用120帧/秒的高帧率。



`AR-Seg`的交替分辨率网络框架用于视频分割，**AR-Seg旨在通过对非关键帧使用低分辨率来减少计算成本；同时为了防止因降采而导致的分割性能下降问题，作者设计了一个交叉分辨率特征融合`CR-eFF`模块，并使用一种新的特征相似性训练`FST`策略进行监督。**

`CReFF`首先利用存储在压缩视频中的运动矢量，将高分辨率关键帧的特征高效融合到低分辨率非关键帧上，以实现更好的空间对齐，并采用局部注意机制有选择地聚合关键帧中的局部特征。此外，提出的`FST`通过显式相似性损失和共享解码层的隐式约束，对聚合后的特征进行监督。



**基于图像的分割方法应用于逐帧视频帧会消耗相当多的计算资源。**

为了提高VSS的计算效率，现有的方法主要集中在网络架构的设计上：

- 一类主流方法是提出了紧凑高效的基于图像的架构来降低每帧的计算开销。
- 另一类主流方法是将深度模型应用到关键帧上，而浅层网络模型则应用于非关键帧上以避免视频帧的重复计算。



考虑到视频分割的特殊性（**相邻帧信息通常是强相关的**），为此本文提出了利用视频中的时间相关性（相邻帧的相关性）来防止降低分辨率带来的精度下降问题。它的核心思想是：**低分辨率帧中缺少的局部特征信息，是可以从稀疏的高分辨率参考帧来获取到的。**

**稀疏相当于目标有效部分较少**

![img](https://picx.zhimg.com/v2-e098fea953c43ed49ff6afa3d6eceedf_r.jpg)

跨分辨率特征融合（CReFF）模块，特征相似性训练（FST）策略对其进行监督,CReFF 首先利用压缩视频中存储的**运动向量将高分辨率的关键帧特征变换到低分辨率的非关键帧上**，以实现更好的空间对齐，然后**通过局部注意力机制选择性地聚合**这些变换后的特征。

![QQ_1738739967714](assets/QQ_1738739967714.png)

FST 策略指导了 CReFF 聚合特征的学习。FST 包括显式的相似性损失（聚合特征与从非关键帧推断出的 HR 特征之间的相似性损失）以及 HR 分支和 LR 分支共享解码层带来的隐式约束。



AR-Seg框架充分利用了压缩视频的特性，具体包括：

- **运动矢量（Motion Vectors, MV）**：压缩视频中的运动矢量用于将高分辨率关键帧的特征对齐到低分辨率非关键帧，从而实现空间对齐。这种方法避免了传统光流计算的高成本。
	- 关键帧（I帧）存储完整图像信息（高分辨率HR）
	- 非关键帧（P/B帧）通过运动补偿编码：(记录着每个画面块的位置变化（如车轮移动轨迹）)
		- 运动矢量：记录宏块（16x16像素）相对于参考帧的位移矢量
		- 残差数据：补偿后的像素差异值
	- a. 运动矢量上采样：通过双线性插值将块级MV转换为像素级位移场
		b. 可变形卷积适配：使用MV生成偏移量参数，动态调整卷积采样位置
		c. 残差补偿机制：结合编码残差数据修正特征差异
- **帧类型划分**：AR-Seg将压缩视频中的I帧作为关键帧，以高分辨率处理；P帧和B帧作为非关键帧，以低分辨率处理。这种划分显著降低了计算量。
- **特征融合**：通过 **CReFF（Cross Resolution Feature Fusion）模块**，将高分辨率关键帧的特征与低分辨率非关键帧的特征融合，弥补了低分辨率带来的信息损失。



 **显式相似性损失（Explicit Similarity Loss）**

- **目标**：P强制低分辨率分支输出的特征与高分辨率分支的特征在语义上对齐，弥补分辨率降低导致的信息损失。这种显式约束的作用是帮助低分辨率模型更好地学习从高分辨率模型中汇聚的信息，并提高分割结果的质量。

**隐式约束指导（Implicit Constraint）**

- **目标**：通过模型结构设计或间接监督信号，引导低分辨率分支自主捕获关键帧的局部细节。
- 通过共享的 1x1 卷积层，*F**p* 被隐式地指导去学习与 *F**I* 相似的特征表示。
- 这就像你用同一个滤镜处理低分辨率照片，确保它在视觉效果上与高分辨率照片一致。
- 相当于由于所有低分辨率都会与同一参数进行计算，但低分辨本就会潜移默化向高分辨率靠近，此时更会提取相同特征也就是高分辨率的特点，更加潜移默化的向高分辨率靠近

**通过明确和隐式的约束，`FST`有效地将HR特征的知识从HR分支传递到LR分支，实现基于`CReFF`的聚合特征的高质量分割**



HR分支用于**关键帧I**，LR分支用于**非关键帧P,B**。

I帧、P帧和B帧是针对视频序列中的不同帧类型，而高分辨率和低分辨率的特征是针对同一帧的不同分辨率处理。因此，**高分辨率和低分辨率的特征并不直接对应于I帧、P帧或B帧**。

**损失计算的对象**

- **高分辨率特征**：从原始分辨率（或较高分辨率）的输入图像中提取的特征 *FP*。
- **低分辨率特征**：从下采样后的输入图像中提取的特征 *F*~*P*。
- **损失计算**：通过计算高分辨率特征 *F**P* 和低分辨率特征 *F*~*P* 之间的差异，优化低分辨率特征，使其更接近高分辨率特征。



**特征相似性损失函数**的目标是让**p低分辨率特征**尽可能接近**p高分辨率特征**。下采样操作是在**输入阶段**对原始视频帧进行处理的（I帧或P帧）的分辨率一致，主动下采样

![img](assets/v2-db3f3e101863daea2f79c4f654780b51_r.jpg)

## 运行流程

- - ![img](assets/{E1A48BD0-CB64-4DBF-84A7-17789DDA354D})
	- ![img](assets/{BB489040-6E37-4B92-A9D0-9FAAE94C01A0})
	- ![img](assets/{41C5DF2C-0157-444E-814E-201F3CACC9FB})
	- ![QQ_1739031425916](assets/QQ_1739031425916.png)

**总结：**

高分辨率图片的参数就像大哥哥一样由于其更加注意细节，就指导指导低分辨率图片去注意一些重要的，也就是注意力机制，低分辨率加权融合使其能够从高分辨率特征中学习到关键信息。加权后的低分辨率特征与高分辨率特征进行融合也就是cerff，生成综合特征。特征融合使得模型能够利用高分辨率特征来增强低分辨率特征，从而提高非关键帧的分割质量。这种融合方式在保证准确性的前提下，进一步降低了计算成本。模型使用**综合特征**进行语义分割，生成每个像素所属的语义类别

## 代码

### 特征提取

- **`ContextPath`**：
	- 提取多尺度的上下文特征，主要关注全局语义信息。
	- 通过逐步降低分辨率和增加通道数，捕获高层次的语义信息。
	- BiSeNet 的 **上下文路径（Context Path）** 会以 ResNet 作为主干网络（Backbone），用于提取高层语义特征。
- **`SpatialPath`**：
	- 提取高分辨率的空间特征，主要关注局部细节信息。
	- 通过逐步降低分辨率和增加通道数，保留更多的空间细节信息。

通过将 `SpatialPath` 和 `ContextPath` 的特征进行融合，模型能够在保持全局语义信息的同时，保留局部细节信息，从而提高语义分割的精度。



### 特征融合方式

在此文中是在通道方向上进行拼接，原因是想使用通道注意力机制

 **特征融合网络（如SENet的Squeeze-and-Excitation）**

- **实现方式**：通过一个小型的子网络来学习特征的权重，通常包括挤压（全局平均池化）、激发（全连接层 + 激活函数）和特征重新缩放。
- **优点**：
	- 能够根据特征的重要性动态调整权重，提高特征的表达能力。
	- 具有较强的泛化能力，适用于多种视觉任务。
- **缺点**：
	- 增加了网络的复杂度和计算成本。
	- 需要更多的训练数据和时间来调整权重。
- **适用场景**：适用于需要对特征进行细粒度调整和优化的场景，如图像分类和目标检测。

 **Transformer融合（如Swin Transformer中的Window Attention）**

- **实现方式**：利用Transformer模型中的自注意力机制或交叉注意力机制来融合特征图，通常涉及将特征图划分为 patches，并通过注意力机制学习 patches 之间的关系。
- **优点**：
	- 具有强大的特征捕捉能力，能够捕捉长距离的特征依赖关系。
	- 可以自动学习特征图之间的复杂关系，无需手工设计参数。
- **缺点**：
	- 计算成本较高，尤其是在处理高分辨率图像时。
	- 可能需要更多的训练数据和时间来调整模型参数。
- **适用场景**：适用于需要捕捉全局特征依赖关系的场景，如语义分割和视频理解。

**渐进式融合（Progressive Fusion）**

- **实现方式**：分阶段融合不同分辨率的特征图，通常涉及通过上采样或下采样操作和卷积层来融合特征。
- **优点**：
	- 可以有效融合多尺度特征，提高模型对不同尺度信息的捕捉能力。
	- 具有较好的灵活性，可以根据不同任务的需求调整融合策略。
- **缺点**：
	- 计算复杂度相对较高。
	- 需要更多的训练时间来调整不同阶段的融合参数。
- **适用场景**：适用于需要捕捉多尺度信息的场景，如目标检测和语义分割。



**特征融合**：通过拼接操作融合了来自不同路径的特征，使模型能够同时利用高分辨率的空间特征和低分辨率的上下文特征。

空间特征获取使用1x1卷积核，上下文特征则使用



### 不同注意力机制

- **空间注意力机制**：

	- **维度**：关注特征图的**空间维度**（高度和宽度）。
	- **目标**：通过计算每个空间位置的重要性，动态调整特征图中各个位置的权重，增强重要位置的特征，抑制不重要位置的特征。
	- **适用场景**：适用于需要增强特征图中某些特定区域的任务，例如目标检测、语义分割、图像修复等，这些任务中不同位置的特征可能对应不同的对象或区域。

	

	- **全局信息提取**：通过全局平均池化（Global Average Pooling）和全局最大池化（Global Max Pooling）将特征图的空间维度压缩为1×1，生成全局特征向量。形状为 `(batch_size, channels, 1, 1)`
	- **生成注意力图**：通过全连接层（或1×1卷积层）生成通道注意力图，并使用Sigmoid函数将权重归一化到[0, 1]区间。
	- **特征加权**：将通道注意力图与原始特征图逐通道相乘，生成加权后的特征图。

- **通道注意力机制**：

	- **维度**：关注特征图的**通道维度**。
	- **目标**：通过计算每个通道的重要性，动态调整特征图中各个通道的权重，增强重要通道的特征，抑制不重要通道的特征。
	- **适用场景**：适用于需要增强特征图中某些通道信息的任务，例如图像分类、目标检测等，这些任务中不同通道的特征可能对应不同的语义信息。

	

	- **通道压缩**：通过全局平均池化和全局最大池化将特征图的通道维度压缩为1，生成两个通道压缩后的特征图。(batch_size, 1, height, width)
	- **生成注意力图**：将通道压缩后的特征图拼接起来，通过卷积层生成空间注意力图，并使用Sigmoid函数将权重归一化到[0, 1]区间。
	- **特征加权**：将空间注意力图与原始特征图逐元素相乘，生成加权后的特征图。

使用不同的特征融合方式对于不同的注意力机制这都是可以随意搭配的

### MyAttention

### 1. **实现跨分辨率特征融合（CReFF）**

`MyAttention` 类通过以下方式实现了跨分辨率特征融合（CReFF）：

#### 1.1 **特征对齐**

- **上采样低分辨率特征**：

	```python
	lr_feat = F.interpolate(lr_feat, (H, W), mode='bilinear', align_corners=True)
	```

	- **意义**：将低分辨率特征 `lr_feat` 上采样到高分辨率特征 `hr_feat` 的尺寸，使两者在空间维度上对齐。这一步确保了低分辨率特征和高分辨率特征能够在相同的空间分辨率下进行交互。

#### 1.2 **特征提取**

- **高分辨率特征的键和值**：

	```python
	hr_value = self.hr_value_conv(hr_feat)
	hr_key = self.hr_key_conv(hr_feat)
	```

	- **意义**：通过卷积操作提取高分辨率特征的键（`hr_key`）和值（`hr_value`），为后续的相似性计算和加权融合做准备。

- **低分辨率特征的查询**：

	```python
	lr_query = self.lr_query_conv(lr_feat)
	```

	- **意义**：通过卷积操作提取低分辨率特征的查询（`lr_query`），用于与高分辨率特征的键进行相似性计算。

#### 1.3 **相似性计算**

- **计算相似性权重**：

	```python
	weight = f_similar(lr_query, hr_key, self.kH, self.kW)
	weight = self.softmax(weight)
	```

	- **意义**：通过 `f_similar` 函数计算低分辨率查询特征 `lr_query` 和高分辨率键特征 `hr_key` 之间的相似性权重，并使用 `softmax` 函数进行归一化。这一步实现了特征之间的显式相似性计算，使低分辨率特征能够学习到高分辨率特征的相似性。

#### 1.4 **特征融合**

- **加权融合高分辨率值特征**：

	```python
	attention_result = f_weighting(hr_value, weight, self.kH, self.kW)
	```

	- **意义**：通过 `f_weighting` 函数根据相似性权重 `weight` 对高分辨率值特征 `hr_value` 进行加权融合，得到融合后的特征 `attention_result`。这一步实现了高分辨率特征对低分辨率特征的增强。

#### 1.5 **最终融合结果**

- **低分辨率特征与融合结果相加**：

	```python
	result = lr_feat + attention_result
	```

	- **意义**：将融合后的特征 `attention_result` 与原始低分辨率特征 `lr_feat` 相加，得到最终的融合结果 `result`。这一步实现了低分辨率特征和高分辨率特征的信息互补，提升了低分辨率特征的表征能力。

### 2. **实现特征相似性训练（FST）**

`MyAttention` 类通过以下方式实现了特征相似性训练（FST）：

#### 2.1 **显式相似性损失**

- **相似性计算**：

	```python
	weight = f_similar(lr_query, hr_key, self.kH, self.kW)
	```

	- **意义**：通过计算低分辨率查询特征 `lr_query` 和高分辨率键特征 `hr_key` 之间的相似性，实现了显式相似性损失。这一步确保了低分辨率分支能够学习到高分辨率分支的特征相似性。

#### 2.2 **隐式约束**

- **共享解码器层**：
	- **意义**：在 `MyAttention` 类中，高分辨率分支和低分辨率分支共享解码器层的参数。通过共享参数，低分辨率分支和高分辨率分支的特征被强制保持一致性，实现了隐式约束。

![QQ_1739084200059](assets/QQ_1739084200059.png)

**代码实现中并没有I，P帧而是一张图片不同分辨率以代替，所以也没有运动矢量之类的**



### 其他

#### 3×3 卷积

| 对比维度     | 3×3 卷积优势                                      | 替代方案缺点                 |
| ------------ | ------------------------------------------------- | ---------------------------- |
| **感受野**   | 堆叠 2 层 3×3 等效于 5×5 卷积（参数减少 28%）     | 大卷积核参数更多             |
| **计算效率** | 更少的计算量（FLOPs 减少 55% vs 5×5）             | 大卷积核计算量呈平方增长     |
| **特征复用** | 通过多层叠加实现非线性组合（ReLU 激活的多次作用） | 单层大卷积非线性表达能力有限 |

 成为现代卷积神经网络最广泛使用的卷积配置。

```
kernel_size=3, stride=1, padding=1, bias=False
```

- 保持输入输出尺寸相同（当 stride=1 时）

	- ```
		out_size = (in_size - 3 + 2 * 1) / 1 + 1 = in_size
		```

- 无偏置项（通常与后续的 BatchNorm 配合使用）

	- BatchNorm 本身包含可学习的缩放（γ）和平移（β）参数，相当于已包含偏置作用
	- 去掉卷积的偏置项可减少冗余参数，提高计算效率
	- 如果后续没有 BatchNorm 层，建议启用偏置项（`bias=True`）

**输入输出尺寸一致的通用条件**为：

```
padding = (kernel_size - 1) / 2
```

并且 `stride = 1`。

| 卷积核大小 (`kernel_size`) | 所需填充值 (`padding`) |
| -------------------------- | ---------------------- |
| 1×1                        | 0                      |
| 3×3                        | 1                      |
| 5×5                        | 2                      |
| 7×7                        | 3                      |

**3×3 卷积侧重局部细节，大卷积核（≥5×5）直接捕获全局上下文长距离依赖建模**

**当是`self.conv2 = conv3x3(out_chan, out_chan)`这样输入与输出一样的通道**

- 每个卷积核通过滑动窗口重新组合相邻像素的信息，增强有效特征、抑制噪声。
- 缓解梯度消失。



#### 激活函数使用

| 场景                   | 是否添加激活函数 | 理由                         |
| ---------------------- | ---------------- | ---------------------------- |
| 中间特征提取层         | ✅ 必须           | 引入非线性，增强模型表达能力 |
| 残差分支最后一层卷积   | ❌ 不添加         | 避免破坏残差相加后的分布     |
| 归一化层后的卷积       | ✅ 在归一化后添加 | 标准化后再激活更稳定         |
| 使用Softmax, Sigmoid等 | ❌ 可选           | 根据任务需求决定             |



`forward_phase1`、`forward_phase2` 和 `forward` 方法的关系解析：

```
Text+----------------------+
|       forward        |  ← 主入口
+----------------------+
  |            |
  |            +---（当 mode='normal'）--→ 直接完成全流程
  |
  +---（当 mode='merge'）--→ 调用:
                             |
                             +--- forward_phase1 → 提取中间特征
                             |
                             +--- forward_phase2 → 融合参考特征并输出
```

#### **`forward_phase1` 流程**

```
输入图像 → Context Path → feat_cp8, feat_cp16
     ↓
输入图像 → Spatial Path → feat_sp → 上采样对齐 → 特征融合 (FFM) → 中间特征 (middle_feat)
                     ↓
                    辅助输出 (feat_out16, feat_out32)
```

#### **`forward_phase2` 流程**

```
中间特征 (middle_feat) + 参考特征 (ref_p) → 注意力融合 → 最终卷积 → 上采样 → 输出 (out)
```


